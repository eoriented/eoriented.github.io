<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam on Smart Tiger&#39;s blog</title>
    <link>https://eoriented.github.io/tags/adam/</link>
    <description>Recent content in Adam on Smart Tiger&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-KR</language>
    <managingEditor>eoriented@gmail.com (eoriented)</managingEditor>
    <webMaster>eoriented@gmail.com (eoriented)</webMaster>
    
	<atom:link href="https://eoriented.github.io/tags/adam/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>딥러닝(Deep Learning) 살펴보기 2탄</title>
      <link>https://eoriented.github.io/post/deep-learning-overview-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>eoriented@gmail.com (eoriented)</author>
      <guid>https://eoriented.github.io/post/deep-learning-overview-2/</guid>
      <description>지난 포스트에 Deep learning 살펴보기 1탄을 통해 딥러닝의 개요와 뉴럴 네트워크, 그리고 Underfitting의 문제점과 해결방법에 관해 알아보았습니다. 그럼 오늘은 이어서 Deep learning에서 학습이 느린 문제점을 어떠한 방식으로 해결하고 연구하고 있는지 한번 알아보도록 하겠습니다.
Neural Network 복습 기존의 뉴럴 네트워크는 weight parameter들을 최적화(optimize)를 하기 위하여 Gradient Descent 방법을 사용했습니다.</description>
    </item>
    
  </channel>
</rss>